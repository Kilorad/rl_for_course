{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7sfWBeBq8Wx"
   },
   "source": [
    "## DQN\n",
    "\n",
    "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GizSqM0ft1Ay",
    "outputId": "bdb61103-71c6-4c01-b305-6e239490ee54"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D7-10joYt1Az"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bfxLPTQt1Az"
   },
   "source": [
    "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRnOxiAZrOFN",
    "outputId": "152a8690-e739-4a04-843e-8f7f8b1d2b2a"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYbIV7w42Fp1"
   },
   "source": [
    "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
    "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOrcN5-at1A0"
   },
   "source": [
    "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
    "$$\n",
    "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] \\\\\n",
    "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
    "$$\n",
    "где\n",
    "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
    "* $\\gamma$ дисконтирующий множитель.\n",
    "\n",
    "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BFkc4eN16Lh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyQ1U2yQt1A1",
    "outputId": "698b0941-4e03-40ac-e3e4-24f4c04219e0"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "print(f'Action_space: {n_actions} \\nState_space: {env.observation_space.shape}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EILvZMY7t1A1"
   },
   "source": [
    "Задавайте небольшой размер скрытых слоев, например не больше 200.\n",
    "Определяем граф вычислений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7epu6GvYt1A1"
   },
   "outputs": [],
   "source": [
    "# TODO: refactor hidden_dims and make it more clear (typing and so on)\n",
    "def create_network(input_dim, hidden_dims, output_dim):\n",
    "    network = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dims[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dims[1], output_dim),\n",
    "    )\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz1KBLVEt1A1"
   },
   "outputs": [],
   "source": [
    "def select_action_eps_greedy(network, state, epsilon):\n",
    "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    Q_s = network(state).detach().numpy()\n",
    "    if epsilon < np.random.random():\n",
    "        action = np.argmax(Q_s)\n",
    "    else:\n",
    "        n_actions = Q_s.shape[-1]\n",
    "        action = np.random.choice(n_actions)\n",
    "\n",
    "    action = int(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3P5cpqVt1A1"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(\n",
    "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
    "):\n",
    "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
    "\n",
    "    # переводим входные данные в тензоры\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "\n",
    "\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
    "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
    "\n",
    "    # получаем значения q для всех действий из текущих состояний\n",
    "    predicted_qvalues = network(states)\n",
    "\n",
    "    # получаем q-values для выбранных действий\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
    "\n",
    "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "\n",
    "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
    "    next_state_values = torch.max(predicted_next_qvalues.detach(), axis=-1)[0]\n",
    "\n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # вычисляем target q-values для функции потерь\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
    "\n",
    "    # для последнего действия в эпизоде используем\n",
    "    # упрощенную формулу Q(s,a) = r(s,a),\n",
    "    # т.к. s' для него не существует\n",
    "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
    "\n",
    "    # MSE loss для минимизации\n",
    "    loss = torch.mean(losses)\n",
    "    # добавляем регуляризацию на значения Q\n",
    "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
    "\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNaqhETxt1A2"
   },
   "source": [
    "## Simple DQN\n",
    "\n",
    "Немного модифицированная версия кода, запускающего обучение Q-learning из прошлой тетрадки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttk3T8-Bt1A2"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzmWG3Hjt1A2"
   },
   "outputs": [],
   "source": [
    "def test_dqn():\n",
    "    lr = .0001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "\n",
    "    env.reset()\n",
    "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
    "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(train_ep_len):\n",
    "        _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4yVom3tt1A2",
    "outputId": "8b51a35b-bbd9-4964-a368-c4aa1d797a59"
   },
   "outputs": [],
   "source": [
    "test_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZEvkVUft1A2"
   },
   "source": [
    "## DQN with Experience Replay\n",
    "\n",
    "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
    "\n",
    "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbGxQWYNt1A2",
    "outputId": "56a8eb8c-04bc-415f-9636-74e2a855a77f"
   },
   "outputs": [],
   "source": [
    "def sample_batch(replay_buffer, n_samples):\n",
    "    # sample randomly `n_samples` samples from replay buffer\n",
    "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
    "    indices = np.random.choice(len(replay_buffer), n_samples)\n",
    "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
    "    for i in indices:\n",
    "        s, a, r, n_s, done = replay_buffer[i]\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_actions.append(n_s)\n",
    "        dones.append(done)\n",
    "\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m6wb8z4t1A2"
   },
   "outputs": [],
   "source": [
    "def generate_session_rb(\n",
    "        env, network, opt, replay_buffer, glob_step,\n",
    "        train_schedule, batch_size,\n",
    "        t_max=300, epsilon=0, train=False\n",
    "):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            # put new sample into replay_buffer\n",
    "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
    "\n",
    "            if replay_buffer and glob_step % train_schedule == 0:\n",
    "                # sample new batch: train_batch = ...\n",
    "                train_batch = sample_batch(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, is_done = train_batch\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        glob_step += 1\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward, glob_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofoj5xrJt1A3"
   },
   "source": [
    "После проверки скорости обучения можете поэкспериментировать с различными `train_schedule`, `batch_size`, а также с размером буфера `replay_buffer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkcF2YEjt1A3"
   },
   "outputs": [],
   "source": [
    "def test_dqn_replay_buffer():\n",
    "    lr = .0001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    train_schedule, batch_size = 4, 32\n",
    "    replay_buffer = deque(maxlen=4000)\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "    glob_step = 0\n",
    "\n",
    "    env.reset()\n",
    "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
    "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    reward_log = []\n",
    "    for ep in range(train_ep_len):\n",
    "        _, glob_step = generate_session_rb(\n",
    "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
    "        )\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew, _ = generate_session_rb(\n",
    "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
    "            )\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "            reward_log.append(running_avg_rew)\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay\n",
    "    return reward_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "2bEIfTsDt1A3",
    "outputId": "99fe561a-19db-40fa-b4c6-c6dfd36c4d5d"
   },
   "outputs": [],
   "source": [
    "reward_log = test_dqn_replay_buffer()\n",
    "plt.plot(reward_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vrhylbvpt1A3"
   },
   "source": [
    "## DQN with Prioritized Experience Replay\n",
    "\n",
    "Добавим каждому переходу, хранящемуся в памяти, значение приоритета. Популярным вариантом является абсолютное значение TD-ошибки.\n",
    "\n",
    "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз - накладно. Приходится искать баланс между точностью и скоростью.\n",
    "\n",
    "Здесь мы будем делать следующее:\n",
    "\n",
    "- использовать TD-ошибку в кач-ве приоритета\n",
    "- после использования батча при обучении, обновляем значения приоритета для этого батча в памяти\n",
    "- будем периодически сортировать память для того, чтобы новые переходы заменяли собой те переходы, у которых наименьшие значения ошибки (т.е. наименьший приоритет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtIUkctjt1A3"
   },
   "outputs": [],
   "source": [
    "def softmax(xs, temp=1000.):\n",
    "    if not isinstance(xs, np.ndarray):\n",
    "        xs = np.array(xs)\n",
    "\n",
    "    # Обрати внимание, насколько большая температура по умолчанию!\n",
    "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
    "    return exp_xs / exp_xs.sum()\n",
    "\n",
    "def sample_prioritized_batch(replay_buffer, n_samples):\n",
    "    # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
    "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
    "    # Also, keep samples' indices (into `indices`) to return them too!\n",
    "    losses = [sample[0] for sample in replay_buffer]\n",
    "    probs = softmax(losses)\n",
    "    indices = np.random.choice(len(replay_buffer), n_samples, p=probs)\n",
    "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
    "    for i in indices:\n",
    "        _, s, a, r, n_s, done = replay_buffer[i]\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_actions.append(n_s)\n",
    "        dones.append(done)\n",
    "\n",
    "    batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)\n",
    "    return batch, indices\n",
    "\n",
    "def update_batch(replay_buffer, indices, batch, new_losses):\n",
    "    \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
    "    states, actions, rewards, next_states, is_done = batch\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        new_batch = new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i]\n",
    "        replay_buffer[indices[i]] = new_batch\n",
    "\n",
    "def sort_replay_buffer(replay_buffer):\n",
    "    \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning\n",
    "    ==> they will be replaced with the new samples earlier.\"\"\"\n",
    "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
    "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
    "    return new_rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGivr1Bbt1A3"
   },
   "outputs": [],
   "source": [
    "def generate_session_prioritized_rb(\n",
    "        env, network, opt, replay_buffer, glob_step,\n",
    "        train_schedule, batch_size,\n",
    "        t_max=300, epsilon=0, train=False\n",
    "):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            # Compute new sample loss (it's the second returning value - `losses` - from compute_td_loss)\n",
    "            # we need `losses.numpy()[0]`\n",
    "            with torch.no_grad():\n",
    "                _, losses = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
    "\n",
    "            # put new sample into replay_buffer\n",
    "            replay_buffer.append((losses.numpy()[0], s, a, r, next_s, terminated and not truncated))\n",
    "\n",
    "            if len(replay_buffer) >= batch_size and (glob_step + 1) % train_schedule == 0:\n",
    "                # sample new batch: train_batch, indices = ...\n",
    "                train_batch, indices = sample_prioritized_batch(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, is_done = train_batch\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # compute updated losses for the training batch and update batch in replay buffer\n",
    "                    _, losses = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
    "                    update_batch(replay_buffer, indices, train_batch, losses.numpy())\n",
    "\n",
    "            # periodically re-sort replay buffer to prioritize replacing with new samples those samples\n",
    "            # that have the least loss\n",
    "            if len(replay_buffer) >= batch_size and (glob_step + 1) % 25*train_schedule == 0:\n",
    "                replay_buffer = sort_replay_buffer(replay_buffer)\n",
    "\n",
    "        glob_step += 1\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward, glob_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tjNHk8jt1A4",
    "outputId": "a4962e81-a9cc-487d-86a1-f941e4736414"
   },
   "outputs": [],
   "source": [
    "def test_dqn_prioritized_replay_buffer():\n",
    "    lr = .0001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    train_schedule, batch_size = 4, 32\n",
    "    replay_buffer = deque(maxlen=4000)\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "    glob_step = 0\n",
    "    reward_log = []\n",
    "\n",
    "    env.reset()\n",
    "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
    "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(train_ep_len):\n",
    "        _, glob_step = generate_session_prioritized_rb(\n",
    "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
    "        )\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew, _ = generate_session_prioritized_rb(\n",
    "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
    "            )\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "            reward_log.append(running_avg_rew)\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay\n",
    "    return(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "D5L7z0rft1A4",
    "outputId": "570fe619-c238-4039-f754-737debbb784d"
   },
   "outputs": [],
   "source": [
    "reward_log2 = test_dqn_prioritized_replay_buffer()\n",
    "plt.plot(reward_log2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5fP3lXit1A4"
   },
   "source": [
    "<h1>Double DQN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_eps_greedy_double(networks, state, epsilon):\n",
    "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    Q_s = np.stack([networks[0](state).detach().numpy(), networks[1](state).detach().numpy()])\n",
    "    Q_s = np.min(Q_s, axis=0)\n",
    "    if epsilon < np.random.random():\n",
    "        action = np.argmax(Q_s)\n",
    "    else:\n",
    "        n_actions = Q_s.shape[-1]\n",
    "        action = np.random.choice(n_actions)\n",
    "\n",
    "    action = int(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_double_dqn(env, networks, opts, t_max=300, epsilon=0, train=False):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy_double(networks, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            for net_num in [0,1]:\n",
    "                opts[net_num].zero_grad()\n",
    "                loss, _ = compute_td_loss(networks[net_num], [s], [a], [r], [next_s], [terminated and not truncated])\n",
    "                loss.backward()\n",
    "                opts[net_num].step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_double_dqn():\n",
    "    lr = .0001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "    reward_log = []\n",
    "\n",
    "    env.reset()\n",
    "    networks = [create_network(env.observation_space.shape[0], [128, 128], env.action_space.n), create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)]\n",
    "    opts = opts = [torch.optim.Adam(networks[0].parameters(), lr=lr), torch.optim.Adam(networks[1].parameters(), lr=lr)]\n",
    "\n",
    "    for ep in range(train_ep_len):\n",
    "        _ = generate_session_double_dqn(env, networks, opts, epsilon=eps, train=True)\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew = generate_session_double_dqn(env, networks, opts, epsilon=eps, train=False)\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "            reward_log.append(running_avg_rew)\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay\n",
    "    return reward_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_log = test_double_dqn()\n",
    "plt.plot(reward_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Monte Carlo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_eps_greedy_mc(network, state, epsilon):\n",
    "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    n_actions = env.action_space.n\n",
    "    action_variants = torch.eye(n_actions)\n",
    "    sa = torch.hstack([torch.vstack([state] * n_actions),action_variants])\n",
    "    Q_s = np.ravel(network(sa).detach().numpy())\n",
    "    if epsilon < np.random.random():\n",
    "        action = np.argmax(Q_s)\n",
    "    else:\n",
    "        n_actions = Q_s.shape[-1]\n",
    "        action = np.random.choice(n_actions)\n",
    "    \n",
    "    action = int(action)\n",
    "    return action\n",
    "def compute_mc_loss(\n",
    "        network, states, actions, disco_rewards, check_shapes=False, regularizer=.1\n",
    "):\n",
    "    \"\"\" Считатет ошибку для монте карло, используя лишь операции фреймворка torch. \"\"\"\n",
    "\n",
    "    # переводим входные данные в тензоры\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
    "    disco_rewards = torch.tensor(disco_rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "\n",
    "\n",
    "    # получаем значения q для всех действий из текущих состояний\n",
    "    actions_onehot = torch.zeros([actions.shape[0], n_actions])\n",
    "    actions_onehot[:, actions] = 1\n",
    "    sa = torch.hstack([states, actions_onehot])\n",
    "    predicted_qvalues_for_actions = network(sa)\n",
    "\n",
    "    losses = (predicted_qvalues_for_actions - disco_rewards) ** 2\n",
    "\n",
    "    # MSE loss для минимизации\n",
    "    loss = torch.mean(losses)\n",
    "    # добавляем регуляризацию на значения Q\n",
    "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
    "\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch_montecarlo(replay_buffer, n_samples, gamma=0.99):\n",
    "    # sample randomly `n_samples` samples from replay buffer\n",
    "    # and split an array of samples into arrays: states, actions, discounted rewards\n",
    "    planning_horizon = 100\n",
    "    indices = np.random.choice(len(replay_buffer), n_samples)\n",
    "    states, actions, disco_rewards = [], [], []\n",
    "    for i in indices:\n",
    "        r_disco = 0\n",
    "        for j in range(planning_horizon):\n",
    "            if j + i >= len(replay_buffer):\n",
    "                break\n",
    "            _, _, r, _, d = replay_buffer[j + i]\n",
    "            if d:\n",
    "                break\n",
    "            r_disco += r * (gamma ** j)\n",
    "        s, a, _, _, _ = replay_buffer[i]\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        disco_rewards.append(r_disco)\n",
    "\n",
    "    return np.array(states), np.array(actions), np.array(disco_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_montecarlo(\n",
    "        env, network, opt, replay_buffer, glob_step,\n",
    "        train_schedule, batch_size,\n",
    "        t_max=300, epsilon=0, train=False\n",
    "):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy_mc(network, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        if train:\n",
    "            # put new sample into replay_buffer\n",
    "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
    "\n",
    "            if replay_buffer and glob_step % train_schedule == 0:\n",
    "                # sample new batch: train_batch = ...\n",
    "                train_batch = sample_batch_montecarlo(replay_buffer, batch_size)\n",
    "                states, actions, disco_rewards = train_batch\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss, _ = compute_mc_loss(network, states, actions, disco_rewards)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        glob_step += 1\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward, glob_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_montecarlo():\n",
    "    lr = .0001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_schedule, batch_size = 4, 32\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    replay_buffer = deque(maxlen=4000)\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "    reward_log = []\n",
    "\n",
    "    env.reset()\n",
    "    network = create_network(env.observation_space.shape[0] + env.action_space.n, [128, 128], 1)\n",
    "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "    glob_step = 0\n",
    "\n",
    "    for ep in range(train_ep_len):\n",
    "        _, glob_step = generate_session_montecarlo(\n",
    "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
    "        )\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew = generate_session_montecarlo(env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False)\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "            reward_log.append(running_avg_rew)\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay\n",
    "    return reward_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_log = test_montecarlo()\n",
    "plt.plot(reward_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dueling Monte Carlo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dmc_loss(\n",
    "        networks, states, actions, disco_rewards, check_shapes=False, regularizer=.1\n",
    "):\n",
    "    \"\"\" Считатет ошибку для dueling монте карло, используя лишь операции фреймворка torch. \"\"\"\n",
    "\n",
    "    # переводим входные данные в тензоры\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
    "    disco_rewards = torch.tensor(disco_rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "\n",
    "    # получаем значения value для текущих состояний\n",
    "    predicted_values = networks[0](states)\n",
    "    losses_values = (predicted_values - disco_rewards) ** 2\n",
    "    losses_values = 5 * torch.mean(losses_values)\n",
    "\n",
    "\n",
    "    advantages = predicted_values.detach() - disco_rewards\n",
    "\n",
    "    # получаем значения q для всех действий из текущих состояний\n",
    "    actions_onehot = torch.zeros([actions.shape[0], n_actions])\n",
    "    actions_onehot[:, actions] = 1\n",
    "    sa = torch.hstack([states, actions_onehot])\n",
    "    predicted_qvalues_for_actions = networks[1](sa)\n",
    "\n",
    "    advantages = torch.stack([advantages] * predicted_qvalues_for_actions.shape[0])\n",
    "    losses_advantage = (predicted_qvalues_for_actions - advantages) ** 2\n",
    "\n",
    "    # MSE loss для минимизации\n",
    "    losses_advantage = torch.mean(losses_advantage)\n",
    "    # добавляем регуляризацию на значения Q\n",
    "    loss = losses_advantage + losses_values + regularizer * predicted_qvalues_for_actions.mean()\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
    "\n",
    "    return loss, losses_advantage\n",
    "def generate_session_dmontecarlo(\n",
    "        env, networks, opts, replay_buffer, glob_step,\n",
    "        train_schedule, batch_size,\n",
    "        t_max=300, epsilon=0, train=False\n",
    "):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy_mc(networks[1], s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        if train:\n",
    "            # put new sample into replay_buffer\n",
    "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
    "\n",
    "            if replay_buffer and glob_step % train_schedule == 0:\n",
    "                # sample new batch: train_batch = ...\n",
    "                train_batch = sample_batch_montecarlo(replay_buffer, batch_size)\n",
    "                states, actions, disco_rewards = train_batch\n",
    "\n",
    "                opts[0].zero_grad()\n",
    "                opts[1].zero_grad()\n",
    "                loss, _ = compute_dmc_loss(networks, states, actions, disco_rewards)\n",
    "                loss.backward()\n",
    "                opts[0].step()\n",
    "                opts[1].step()\n",
    "\n",
    "        glob_step += 1\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return total_reward, glob_step\n",
    "def test_dmontecarlo():\n",
    "    lr = .001\n",
    "    eps, eps_decay = .5, .998\n",
    "    train_schedule, batch_size = 5, 128\n",
    "    train_ep_len, eval_schedule = 10000, 50\n",
    "    replay_buffer = deque(maxlen=4000)\n",
    "    eval_rewards = deque(maxlen=5)\n",
    "    reward_log = []\n",
    "\n",
    "    env.reset()\n",
    "    networks = [create_network(env.observation_space.shape[0], [128, 128], 1), create_network(env.observation_space.shape[0] + env.action_space.n, [128, 128], 1)]\n",
    "    opts = [torch.optim.Adam(networks[0].parameters(), lr=lr), torch.optim.Adam(networks[1].parameters(), lr=lr)]\n",
    "    glob_step = 0\n",
    "\n",
    "    for ep in range(train_ep_len):\n",
    "        _, glob_step = generate_session_dmontecarlo(\n",
    "            env, networks, opts, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
    "        )\n",
    "\n",
    "        if (ep + 1) % eval_schedule == 0:\n",
    "            ep_rew = generate_session_dmontecarlo(env, networks, opts, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False)\n",
    "            eval_rewards.append(ep_rew)\n",
    "            running_avg_rew = np.mean(eval_rewards)\n",
    "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "            reward_log.append(running_avg_rew)\n",
    "\n",
    "            if eval_rewards and running_avg_rew >= 200.:\n",
    "                print(\"Принято!\")\n",
    "                break\n",
    "\n",
    "        eps *= eps_decay\n",
    "    return reward_log\n",
    "reward_log = test_dmontecarlo()\n",
    "plt.plot(reward_log)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
