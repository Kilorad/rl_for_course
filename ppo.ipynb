{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lGZSpv_ZZMl"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "В практической реализации существует два варианта реализации алгоритма PPO:\n",
        "* выполняет обновление, ограниченное KL, как TRPO, но штрафует KL-расхождение в целевой функции вместо того, чтобы делать его жестким ограничением, и автоматически регулирует коэффициент штрафа в процессе обучения, чтобы он масштабировался соответствующим образом.\n",
        "* не содержит в целевой функции члена KL-дивергенции и вообще не имеет ограничения. Вместо этого полагается на специализированный клиппинг\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg\">\n",
        "\n",
        "Спойлер: клиппинг - не самое главное в PPO, как это могло показаться на первый взгляд. Алгоритм PPO работает во многом и за счет небольших дополнительных улучшений. Подробнее: https://arxiv.org/pdf/2005.12729.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMBoV3wUZZMp"
      },
      "source": [
        "# Устанавливаем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3t4TxHsZZMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dccc826-df6c-4fe8-e1d5-610d5601b6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !apt -qq update -y\n",
        "    !apt -qq install swig -y\n",
        "    !pip -q install box2d-py\n",
        "    !pip -q install \"gymnasium[classic-control, box2d, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85astBbBZZMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Память"
      ],
      "metadata": {
        "id": "1SRC_b_9c8yB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPunJDSFZZMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa0ffca-73f9-4cf3-b113-6e116f819d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bT5JGY_ZZMs"
      },
      "source": [
        "# Сеть Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqYfrssoZZMs"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor: 2 hidden + output\n",
        "        # self.action_layer = nn.Sequential(..., nn.Softmax(dim=-1))\n",
        "        # 3 linear layers\n",
        "        self.action_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # critic: 2 hidden + output\n",
        "        # self.value_layer = nn.Sequential(...)\n",
        "        # 3 linear layers\n",
        "        self.value_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        # сохраняем в память: state, action, log_prob(action)\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6sW_qySZZMt"
      },
      "source": [
        "# PPO policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFdZvsDPZZMt"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo оценка вознаграждений:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            # обнуляем накопленную награду, если попали в терминальное состояние\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + self.gamma * discounted_reward\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        # выполните нормализацию вознаграждений (r - mean(r)) / std(r + 1e-5):\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # конвертация list в tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # оптимизация K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # получаем logprobs, state_values, dist_entropy от стратегии:\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # находим отношение стратегий (pi_theta / pi_theta__old), через logprobs и old_logprobs.detach():\n",
        "            ratios = torch.exp(logprobs) / torch.exp(old_logprobs)\n",
        "\n",
        "            # считаем advantages\n",
        "            advantages = rewards - state_values\n",
        "\n",
        "            # Находим surrogate loss:\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # делаем шаг градиента\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # копируем веса\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isnuAIhPZZMu"
      },
      "source": [
        "# Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvRDe11DZZMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad60d95-6afc-40ca-92fb-3187c93bea37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001 (0.9, 0.999)\n",
            "Episode 20 \t avg length: 92 \t reward: -223\n",
            "Episode 40 \t avg length: 91 \t reward: -180\n",
            "Episode 60 \t avg length: 93 \t reward: -192\n",
            "Episode 80 \t avg length: 89 \t reward: -150\n",
            "Episode 100 \t avg length: 97 \t reward: -138\n",
            "Episode 120 \t avg length: 106 \t reward: -181\n",
            "Episode 140 \t avg length: 105 \t reward: -156\n",
            "Episode 160 \t avg length: 114 \t reward: -178\n",
            "Episode 180 \t avg length: 105 \t reward: -145\n",
            "Episode 200 \t avg length: 105 \t reward: -122\n",
            "Episode 220 \t avg length: 96 \t reward: -119\n",
            "Episode 240 \t avg length: 99 \t reward: -118\n",
            "Episode 260 \t avg length: 101 \t reward: -116\n",
            "Episode 280 \t avg length: 120 \t reward: -130\n",
            "Episode 300 \t avg length: 114 \t reward: -128\n",
            "Episode 320 \t avg length: 125 \t reward: -112\n",
            "Episode 340 \t avg length: 118 \t reward: -123\n",
            "Episode 360 \t avg length: 137 \t reward: -100\n",
            "Episode 380 \t avg length: 119 \t reward: -90\n",
            "Episode 400 \t avg length: 116 \t reward: -94\n",
            "Episode 420 \t avg length: 115 \t reward: -88\n",
            "Episode 440 \t avg length: 118 \t reward: -61\n",
            "Episode 460 \t avg length: 123 \t reward: -74\n",
            "Episode 480 \t avg length: 124 \t reward: -55\n",
            "Episode 500 \t avg length: 118 \t reward: -59\n",
            "Episode 520 \t avg length: 115 \t reward: -44\n",
            "Episode 540 \t avg length: 140 \t reward: -39\n",
            "Episode 560 \t avg length: 139 \t reward: -38\n",
            "Episode 580 \t avg length: 124 \t reward: -47\n",
            "Episode 600 \t avg length: 131 \t reward: -21\n",
            "Episode 620 \t avg length: 198 \t reward: -39\n",
            "Episode 640 \t avg length: 312 \t reward: -42\n",
            "Episode 660 \t avg length: 334 \t reward: -87\n",
            "Episode 680 \t avg length: 444 \t reward: -1\n",
            "Episode 700 \t avg length: 322 \t reward: 6\n",
            "Episode 720 \t avg length: 345 \t reward: 43\n",
            "Episode 740 \t avg length: 196 \t reward: 20\n",
            "Episode 760 \t avg length: 260 \t reward: 9\n",
            "Episode 780 \t avg length: 206 \t reward: 10\n",
            "Episode 800 \t avg length: 228 \t reward: 34\n",
            "Episode 820 \t avg length: 345 \t reward: 51\n",
            "Episode 840 \t avg length: 423 \t reward: 81\n",
            "Episode 860 \t avg length: 472 \t reward: 78\n",
            "Episode 880 \t avg length: 352 \t reward: 49\n",
            "Episode 900 \t avg length: 310 \t reward: 41\n",
            "Episode 920 \t avg length: 417 \t reward: 81\n",
            "Episode 940 \t avg length: 443 \t reward: 73\n",
            "Episode 960 \t avg length: 398 \t reward: 52\n",
            "Episode 980 \t avg length: 323 \t reward: 64\n",
            "Episode 1000 \t avg length: 257 \t reward: 28\n",
            "Episode 1020 \t avg length: 296 \t reward: 51\n",
            "Episode 1040 \t avg length: 318 \t reward: 46\n",
            "Episode 1060 \t avg length: 312 \t reward: 11\n",
            "Episode 1080 \t avg length: 308 \t reward: 61\n",
            "Episode 1100 \t avg length: 313 \t reward: 74\n",
            "Episode 1120 \t avg length: 310 \t reward: 66\n",
            "Episode 1140 \t avg length: 440 \t reward: 112\n",
            "Episode 1160 \t avg length: 398 \t reward: 84\n",
            "Episode 1180 \t avg length: 415 \t reward: 115\n",
            "Episode 1200 \t avg length: 326 \t reward: 78\n",
            "Episode 1220 \t avg length: 422 \t reward: 114\n",
            "Episode 1240 \t avg length: 407 \t reward: 103\n",
            "Episode 1260 \t avg length: 424 \t reward: 119\n",
            "Episode 1280 \t avg length: 468 \t reward: 140\n",
            "Episode 1300 \t avg length: 470 \t reward: 131\n",
            "Episode 1320 \t avg length: 435 \t reward: 110\n",
            "Episode 1340 \t avg length: 493 \t reward: 120\n",
            "Episode 1360 \t avg length: 473 \t reward: 97\n",
            "Episode 1380 \t avg length: 484 \t reward: 113\n",
            "Episode 1400 \t avg length: 474 \t reward: 116\n",
            "Episode 1420 \t avg length: 497 \t reward: 143\n",
            "Episode 1440 \t avg length: 455 \t reward: 135\n",
            "Episode 1460 \t avg length: 484 \t reward: 144\n",
            "Episode 1480 \t avg length: 456 \t reward: 139\n",
            "Episode 1500 \t avg length: 442 \t reward: 137\n",
            "Episode 1520 \t avg length: 424 \t reward: 110\n",
            "Episode 1540 \t avg length: 412 \t reward: 113\n",
            "Episode 1560 \t avg length: 417 \t reward: 131\n",
            "Episode 1580 \t avg length: 403 \t reward: 154\n",
            "Episode 1600 \t avg length: 413 \t reward: 195\n",
            "Episode 1620 \t avg length: 431 \t reward: 183\n",
            "########## Принято! ##########\n"
          ]
        }
      ],
      "source": [
        "# env_name = \"CartPole-v1\"\n",
        "env_name = \"LunarLander-v2\"\n",
        "# env_name = \"MountainCar-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "render = False\n",
        "solved_reward = 200  # останавливаемся если avg_reward > solved_reward\n",
        "log_interval = 20  # печатаем avg reward  в интервале\n",
        "max_episodes = 50000  # количество эпизодов обучения\n",
        "max_timesteps = 500  # максимальное кол-во шагов в эпизоде\n",
        "n_latent_var = 64  # кол-во переменных в скрытых слоях\n",
        "update_timestep = 2000  # обновляем policy каждые n шагов\n",
        "lr = 0.001 # learning rate\n",
        "betas = (0.9, 0.999) # betas для adam optimizer\n",
        "gamma = 0.99  # discount factor\n",
        "K_epochs = 4  # количество эпох обноеления policy\n",
        "eps_clip = 0.1  # clip параметр для PPO\n",
        "random_seed = None\n",
        "\n",
        "if random_seed:\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "print(lr, betas)\n",
        "\n",
        "# переменные для логирования\n",
        "running_reward = 0\n",
        "avg_length = 0\n",
        "timestep = 0\n",
        "\n",
        "# цикл обучения\n",
        "for i_episode in range(1, max_episodes + 1):\n",
        "    state, _ = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        timestep += 1\n",
        "\n",
        "        # используем policy_old для выбора действия\n",
        "        action = ppo.policy_old.act(state, memory)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # сохраняем награды и флаги терминальных состояний:\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(terminated)\n",
        "\n",
        "        # выполняем обновление\n",
        "        if timestep % update_timestep == 0:\n",
        "            ppo.update(memory)\n",
        "            memory.clear_memory()\n",
        "            timestep = 0\n",
        "\n",
        "        running_reward += reward\n",
        "        if render:\n",
        "            env.render()\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    avg_length += t\n",
        "\n",
        "    # останавливаемся, если avg_reward > solved_reward\n",
        "    if running_reward > (log_interval * solved_reward):\n",
        "        print(\"########## Принято! ##########\")\n",
        "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
        "        break\n",
        "\n",
        "    # логирование\n",
        "    if i_episode % log_interval == 0:\n",
        "        avg_length = int(avg_length / log_interval)\n",
        "        running_reward = int((running_reward / log_interval))\n",
        "\n",
        "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "        running_reward = 0\n",
        "        avg_length = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fTg9w63ZZMv"
      },
      "source": [
        "### Дополнительное задание: Попробуйте обучить алгоритм PPO, используя более сложную среду (например LunarLander-v2), для этого вам потребуется подобрать некоторые гиперпараметры.\n",
        "\n",
        "**Работа со средой LunarLander в colab - установка pybox2d:**\n",
        "1. Устанавливаем пакеты:\n",
        "```bash\n",
        "!apt-get install swig -y\n",
        "!pip install gymnasium[box2d]\n",
        "```\n",
        "2. Перезапускаем runtime."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Heg2zGS2obfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}